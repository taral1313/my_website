<!DOCTYPE html>
<html>
<head>
	<title>Post Name</title>


	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1" />

	<link href="https://fonts.googleapis.com/css2?family=Russo+One&display=swap" rel="stylesheet">

	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@500&display=swap" rel="stylesheet">

	<link rel="stylesheet" type="text/css" href="default.css">


	<style type="text/css">
		.img-container{
			max-width: 600px;
		}

		img{
			width: 100%
		}

	</style>

</head>
<body>
	<div class="nav-wrapper">

		<!-- Link around dots-wrapper added after tutorial video -->
		<a href="index.html">
			<div class="dots-wrapper">
				<div id="dot-1" class="browser-dot"></div>
				<div id="dot-2" class="browser-dot"></div>
				<div id="dot-3" class="browser-dot"></div>
			</div>
		</a>

		<ul id="navigation">
			<li><a href="index.html#contact">Contact</a></li>

		</ul>
	</div>

	<div class="main-container">
		<br>

		<div class="img-container">
			<img src="images/automatic_music_gui.png">
		</div>
		
		<h3>Automatic Music Generation using GAN</h3>

		<p>B. Preparing the data:<br><br>

The format of input which is basically the MIDI files of different music which is read by Music21 to generate objects of notes and chords. This data of chords and notes is then fed into our LSTM[x4] network. Each midi is parsed through the Music21 converter. On parsing MIDI files we get a stream object which consists of all notes and chords. The pitch value is encoded into string notation and appended. We encode id of all notes in a chord separating them by a dot in a single string. Output of the network can be easily decoded into notes and chords due to such encoding.

		<br>
		<br>
			GAN:<br>
		 GAN (generative adversarial network) is an artificial intelligence algorithm for unsupervised learning. It was proposed by Goodfellow et al. in 2014. GAN is constituted by two networks, generation network G and discriminator network D, G is responsible for generating target objects. D is responsible for discriminating the object generated by the generator from the real object. With the separate training of the two networks and play zero-sum games with each other in the adaptation training. The generator will generate objects that are very similar to the genuine objects, so that the discriminator network cannot distinguish between the generated objects and the real objects, thereby achieving GAN training.

		<br>
		<br>
		We created the respective MIDI files into numbers to be fed to the model. The model was trained based on the sequence of all the data passed to it.
		<br>
		<br>
		Based on the data, it generates array of float values which when converted to corresponding notes gives a generated music.
		<br>
		<br>
		The generated music is still raw an we more preprocessing and post processing we can achieve amazing results</p>

		<h5>Technologies:</h5>
		<ul>
			<li>- Python</li>
			<li>- TensorFlow</li>
			<li>- Music21</li>
			<li>- Keras</li>
			<li>- Tkinter</li>
			<li></li>
			<li></li>
			<li></li>
		</ul>


		<h5>GUI</h5>
		<div class="img-container">
			<img src="images/gan1.png">
		</div>

		<h6>GAN Architecture</h5>
		<div class="img-container">
			<img src="images/gan3.png">
		</div>

		<h5>Notes Generated</h5>
		<div class="img-container">
			<img src="images/gan2.png">
		</div>

		<h5>PDF Report </h5>
		<div class="img-container">
			<img src="images/gan4.png">
		</div>

		<h5>Interface</h5>
		<div class="img-container">
			<img src="images/automatic_music_gui.png">
		</div>

	</div>

</body>
</html>